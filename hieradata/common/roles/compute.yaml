---
include:
  default:
    - profile::openstack::compute::migration
    - profile::openstack::compute::hypervisor
    - profile::openstack::volume
    - profile::openstack::network
    - profile::logging::rsyslog::client
    - profile::openstack::openrc
    - profile::application::rngd
    - profile::rhsm::subscription
    - profile::rhsm::virtwho
    - profile::base::httpproxy
    - profile::storage::cephclient
  kickstart:
    - profile::virtualization::nested
    - profile::network::yum_proxy
    - profile::monitoring::sensu::agent
    - profile::base::httpproxy
  bootstrap:
    - profile::network::yum_proxy
    - profile::base::httpproxy

# Set SELinux mode to enforcing
profile::base::selinux::manage_selinux: true

# Turn on nested virtualization as default
profile::virtualization::nested::manage: true

# Fix SELinux file context for Nova's .ssh
profile::base::selinux::fcontext:
  nova-dot-ssh:
    seltype:  'ssh_home_t'
    pathspec: '/var/lib/nova/\.ssh(/.*)?'
profile::base::selinux::exec_restorecon:
  nova-dot-ssh:
    path:    '/var/lib/nova/.ssh'
    recurse: true
profile::base::selinux::semodules:
  virtlogd-nova:
    avc_file:   'virtlogd_nova.avc'
  collectd-compute:
    avc_file:   'collectd_compute.avc'

# To fix problem with file descriptor limits in nova-compute
# FIXME: Default false in post yoga and it is deprecated in 2025.1 release
nova::rabbit_heartbeat_in_pthread:                false

# Manage default availability zone
profile::openstack::compute::manage_az:            true

# Manage default custom nova config
profile::openstack::compute::manage_nova_config:   true

# New interface class needs enabling
profile::network::interface::create_custom_routes: true
profile::network::interface::create_ip_rules: true

profile::base::httpproxy::manage_httpproxy: true
profile::network::yum_proxy::yum_proxy:   "http://%{hiera('mgmt__address__proxy')}:8888"

profile::openstack::compute::manage_check_dhcp_lease_file: true

# enable rng daemon for better entropy
profile::application::rngd::enable:       true

# Source proxy for internet access during puppetrun
#puppet::cron_cmd: "if [ -e /root/proxy.sh ] ; then source /root/proxy.sh ; fi ; \
#  /usr/bin/env puppet agent --config %{puppet::dir}/puppet.conf --onetime --no-daemonize"
puppet::cron_cmd: "if [ -e /root/proxy.sh ] ; then source /root/proxy.sh ; fi ; \
  /opt/puppetlabs/bin/puppet agent --config /etc/puppetlabs/puppet/puppet.conf --onetime --no-daemonize"

profile::openstack::compute::migration::enable_nova_account: true
# Set .ssh dir for nova to 0755 to enable sensu checks
profile::openstack::compute::migration::chmod_ssh_dir:       true
# Enable intance migration via ssh
profile::firewall::pre::ssh_settings:
  source: "0.0.0.0/0" #FIXME (limit source)
# SNMP only from mgmt
profile::base::dell::snmp_firewall_settings:
  source: "%{::network_mgmt1}/%{::netmask_mgmt1}"
# compute nodes write firewall rules on the fly, thou shalt not purge
profile::base::firewall::purge_firewall: false

openstack_extras::repo::redhat::redhat::manage_rdo: false
profile::base::common::manage_lvm: true

profile::openstack::openrc::password:     "%{hiera('nova_api_password')}"
profile::openstack::openrc::username:     'nova'
profile::openstack::openrc::project_name: 'services'

profile::openstack::network::plugin: calico

profile::openstack::network::calico::manage_etcd_grpc_proxy: true
profile::openstack::network::calico::manage_etcd:            true
profile::openstack::network::calico::manage_dhcp_agent:      true

profile::base::common::packages:
  'networking-calico': {}
  'bash-completion': {}
  'jq': {}
  'openstack-selinux': {}
  'nrec-selinux': {}

# FIXME:sensu-go
profile::monitoring::sensu::agent::enable_go_agent:   true
profile::monitoring::sensu::agent::enable_agent:      false

# Sensu
sensu::agent::annotations:
  'disk_warning': '90'
  'disk_critical': '95'

sensu::agent::subscriptions:
  - 'base'
  - 'metrics'
  - 'compute'
  - 'physical'

sensu::agent::service_env_vars:
  PATH: '/usr/local/bin:/bin:/usr/bin:/opt/puppetlabs/bin:/opt/sensu-plugins-ruby/embedded/bin'
  HTTPS_PROXY: "%{hiera('mgmt__address__proxy')}:8888"


sudo::configs:
  sensu_tty:
    priority:   20
    content:    'Defaults:sensu !requiretty'
  sensu:
    priority:   25
    content:    'sensu ALL = (root) NOPASSWD: /opt/sensu/embedded/bin/check-file-missing.rb -f /var/lib/nova/.ssh/id_rsa'
  sensu_dhcp:
    priority:   30
    content:
      - [ 'sensu ALL = (root) NOPASSWD: /bin/virsh list', 'sensu ALL = (root) NOPASSWD: /bin/systemctl restart calico-dhcp-agent' ]

profile::openstack::compute::hypervisor::fix_snapshot_loc: true
profile::openstack::compute::hypervisor::sysctl_fixes: true

profile::openstack::telemetry::polling::polling_interval:  3600 # FIXME

calico::compute: true
etcd::mode: 'proxy'

calico::compute::bird_template:  'profile/bird/bird.conf.%{operatingsystemmajrelease}.erb'
calico::compute::bird6_template: 'profile/bird/bird6.conf.erb'
# FIXME workaround that should be fixed upstream in calico-felix
calico::compute::felix_vxlan_port: '31439'

named_interfaces::config:
  mgmt:
    - em3
  trp:
    - team1
  live:
    - team1.120

# compute hosts should have resolving alternatives in case mgmt is down etc
profile::base::network::network_auto_opts:
  mgmt:
    'dns1':     "%{hiera('netcfg_dns_trp_server1')}"
    'dns2':     "%{hiera('netcfg_dns_mgmt_server1')}"

sudo::purge: false

profile::base::lvm::physical_volume:
  '/dev/sda3':
    ensure: present
    force:  true
profile::base::lvm::volume_group:
  'vg_ext':
    physical_volumes:
      - /dev/sda3
profile::base::lvm::logical_volume:
  'lv_instances':
    volume_group: 'vg_ext'
    fs_type:      "xfs"
    mountpath:    "/var/lib/nova/instances"

# Enable extra yum repo
profile::base::yumrepo::repo_hash:
  rdo-release:
    ensure: present
#    exclude: 'openstack-neutron openstack-neutron-common python-neutron'
  calico37:
    ensure: absent
    exclude: 'dnsmasq'
  calico%{hiera('calico_version')}:
    ensure: present
  ceph-%{hiera('ceph_version')}:
    ensure: present
  ceph-%{hiera('ceph_version')}-noarch:
    ensure: present
  AlmaLinux-PowerTools:
    ensure: present
  CentOS-Advanced-Virtualization:
    ensure: absent
  CentOS-AppStream: # needed for rhn network
    ensure:  present
    includepkgs: 'virt-who'
  CentOs-nfv-Openvswitch: # needed for rdo network dependencies
    ensure:  present
  epel: # should be absent when using RDO, but whitelist can also work
    ensure:   present
    includepkgs: 'htop bash-completion-extras bird bird6 collectd collectd-virt collectd-disk lttng-ust userspace-rcu libbabeltrace leveldb liboath py-bcrypt python2-bcrypt python36-prettytable python36-requests python36-pyOpenSSL python36-bcrypt python36-six python36-PyYAML python36-dateutil python36-cryptography python36-urllib3 python36-chardet python36-cffi python36-idna python36-pysocks python36-pycparser python36-ply python36-asn1crypto python3-httplib2 fmt python3-cherrypy python3-zc-lockfile python3-portend python3-cheroot python3-trustme python3-tempora python3-jaraco-functools python3-jaraco parallel thrift'

# Compute nodes need only admin and cinder keys
ceph::profile::params::client_keys:
  'client.admin':
    secret: "%{hiera('ceph_storage_client_admin_key')}"
    mode: '0600'
    user: 'ceph'
    group: 'ceph'
    cap_mon: 'allow *'
    cap_osd: 'allow *'
    cap_mds: 'allow *'
  'client.cinder':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'nova'
    group: 'nova'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'
  'client.cinder-hdd-ec':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=volumes_hdd_ec_meta, profile rbd pool=volumes_hdd_ec_data'
  'client.cinder-ssd-ec':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=volumes_ssd_ec_meta, profile rbd pool=volumes_ssd_ec_data'
  'client.cinder-dcache-hdd-ec':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=dcache_hdd_ec_meta, profile rbd pool=dcache_hdd_ec_data'
  'client.cinder-dcache-hdd-ec82':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=dcache_hdd_ec82_meta, profile rbd pool=dcache_hdd_ec82_data'

profile::storage::cephclient::create_extraconf: true
profile::storage::ceph_extraconf::config:
  client_ecc_backend:
    'config_key'  : "client.cinder-hdd-ec/rbd_default_data_pool"
    'config_value': "volumes_hdd_ec_data"
  client_ssd_ecc_backend:
    'config_key'  : "client.cinder-ssd-ec/rbd_default_data_pool"
    'config_value': "volumes_ssd_ec_data"
  client_dcache_backend:
    'config_key'  : "client.cinder-dcache-hdd-ec/rbd_default_data_pool"
    'config_value': "dcache_hdd_ec_data"
  client_dcache_backend_82:
    'config_key'  : "client.cinder-dcache-hdd-ec82/rbd_default_data_pool"
    'config_value': "dcache_hdd_ec82_data"

# Configuration items not explicitly supported by the Puppet module
nova::config::nova_config:
  DEFAULT/metadata_workers:
    value: '4'

# Red Hat Subscription Management (RHSM)
profile::rhsm::subscription::manage: true
profile::rhsm::virtwho::manage:      true

profile::storage::cephclient::enable: true

openstack_version: 'yoga'

# This creates /etc/openstack/puppet/admin-clouds.yaml
keystone::bootstrap::bootstrap:        false
keystone::bootstrap::manage_resources: false
keystone::bootstrap::username:         'nova'
keystone::bootstrap::password:         "%{hiera('nova_api_password')}"
keystone::bootstrap::project_name:     "%{hiera('keystone__service__project')}"
keystone::bootstrap::internal_url:     "%{hiera('endpoint__identity__internal')}"
keystone::bootstrap::region:           "%{location}"
keystone::bootstrap::interface:        'internal'
