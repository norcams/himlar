---
include:
  default:
    - profile::openstack::compute::migration
    - profile::openstack::compute::hypervisor
    - profile::openstack::volume
    - profile::openstack::network
    - profile::logging::rsyslog::client
    - profile::openstack::openrc
    - profile::application::rngd
    - profile::rhsm::subscription
    - profile::rhsm::virtwho
  kickstart:
    - profile::base::lvm
    - profile::virtualization::nested
    - profile::network::yum_proxy
    - profile::monitoring::sensu::agent
  bootstrap:
    - profile::base::lvm
    - profile::network::yum_proxy

# Set SELinux mode to enforcing
profile::base::selinux::manage_selinux: true

# Turn on nested virtualization as default
profile::virtualization::nested::manage: true

# Fix SELinux file context for Nova's .ssh
profile::base::selinux::fcontext:
  nova-dot-ssh:
    seltype:  'ssh_home_t'
    pathspec: '/var/lib/nova/\.ssh(/.*)?'
profile::base::selinux::exec_restorecon:
  nova-dot-ssh:
    path:    '/var/lib/nova/.ssh'
    recurse: true

# Manage default availability zone
profile::openstack::compute::manage_az:            true

profile::base::network::manage_httpproxy: true
profile::network::yum_proxy::yum_proxy:   "http://%{hiera('mgmt__address__proxy')}:8888"

profile::openstack::compute::manage_check_dhcp_lease_file: true

# enable rng daemon for better entropy
profile::application::rngd::enable:       true

# Source proxy for internet access during puppetrun
#puppet::cron_cmd: "if [ -e /root/proxy.sh ] ; then source /root/proxy.sh ; fi ; \
#  /usr/bin/env puppet agent --config %{puppet::dir}/puppet.conf --onetime --no-daemonize"
puppet::cron_cmd: "if [ -e /root/proxy.sh ] ; then source /root/proxy.sh ; fi ; \
  /opt/puppetlabs/bin/puppet agent --config /etc/puppetlabs/puppet/puppet.conf --onetime --no-daemonize"

profile::openstack::compute::migration::enable_nova_account: true
# Set .ssh dir for nova to 0755 to enable sensu checks
profile::openstack::compute::migration::chmod_ssh_dir:       true
# Enable intance migration via ssh
profile::firewall::pre::ssh_settings:
  source: "0.0.0.0/0" #FIXME (limit source)
# SNMP only from mgmt
profile::base::dell::snmp_firewall_settings:
  source: "%{::network_mgmt1}/%{::netmask_mgmt1}"
# compute nodes write firewall rules on the fly, thou shalt not purge
profile::base::firewall::purge_firewall: false

openstack_extras::repo::redhat::redhat::manage_rdo: false
profile::base::common::manage_lvm: true

profile::openstack::openrc::password:     "%{hiera('nova_api_password')}"
profile::openstack::openrc::username:     'nova'
profile::openstack::openrc::project_name: 'services'

profile::openstack::network::plugin: calico

profile::openstack::network::calico::manage_etcd_grpc_proxy: true
profile::openstack::network::calico::manage_etcd:            true
profile::openstack::network::calico::manage_dhcp_agent:      true

profile::base::common::packages:
  'networking-calico': {}
  'gcc-c++': {}
  'bash-completion': {}
  'bash-completion-extras': {}
  'jq': {}
  'openstack-selinux': {}

# Sensu (needs gcc-c++)
profile::monitoring::sensu::agent::plugins:
  sensu-plugins-http:
    type:         package
    pkg_version:  '2.8.4'
  sensu-plugins-filesystem-checks:
    type:         package
    pkg_version:  '1.0.0'

profile::monitoring::sensu::agent::plugin_gems:
  sensu-plugins-himlar:
    ensure:   '0.4.0'
    provider: 'sensuclassic_gem'
    source:   'http://download.iaas.uio.no/uh-iaas/gem'

# sensu
sensu__memory__check:     "check-memory.rb -w 4096 -c 1024"
sensu__disk__warning:     90
sensu__disk__crtical:     95

profile::monitoring::sensu::agent::checks:
  'metadata-api':
    command:      "check-http.rb -u http://127.0.0.1:8775"
    interval:     60
    subscribers:  ['checks']
  'etcd-client-health':
    command:      "check-http.rb -u http://127.0.0.1:2379/health -q 'true' --response-code 405"
    interval:     60
    subscribers:  ['checks']
  'nova-ssh-key':
    command:      "sudo /opt/sensu/embedded/bin/check-file-missing.rb -f /var/lib/nova/.ssh/id_rsa"
    interval:     60
    subscribers:  ['checks']
  # 'first-lease-file-check':
  #   command:      '/usr/local/bin/check-dhcp-lease-file.sh first 14400 7200' # 4 and 2 hours
  #   interval:     60
  #   subscribers:  ['checks']
  # 'last-lease-file-check':
  #   command:      '/usr/local/bin/check-dhcp-lease-file.sh last 14400 7200' # 4 and 2 hours
  #   interval:     60
  #   subscribers:  ['checks']
  'metrics-disk':
    type:         'metric'
    command:      'metrics-disk-usage.rb --scheme disk.%{::hostname} -f -B G'
    interval:     60
    subscribers:  ['metrics']
    handlers:     ['graphite_tcp']

sudo::configs:
  sensu_tty:
    priority:   20
    content:    'Defaults:sensu !requiretty'
  sensu:
    priority:   25
    content:    'sensu ALL = (root) NOPASSWD: /opt/sensu/embedded/bin/check-file-missing.rb -f /var/lib/nova/.ssh/id_rsa'


profile::openstack::compute::hypervisor::fix_snapshot_loc: true # FIXME - Should probably be removed for newton release

profile::openstack::telemetry::polling::polling_interval:  3600 # FIXME

calico::compute: true
etcd::mode: 'proxy'

calico::compute::bird_template:  'profile/bird/bird.conf.erb'
calico::compute::bird6_template: 'profile/bird/bird6.conf.erb'
# FIXME workaround that should be fixed upstream in calico-felix
calico::compute::felix_vxlan_port: '31439'

named_interfaces::config:
  mgmt:
    - em3
  trp:
    - team1
  live:
    - team1.120

sudo::purge: false

profile::base::lvm::physical_volume:
  '/dev/sda3':
    ensure: present
    force:  true
profile::base::lvm::volume_group:
  'vg_ext':
    physical_volumes:
      - /dev/sda3
profile::base::lvm::logical_volume:
  'lv_instances':
    volume_group: 'vg_ext'
    fs_type:      "xfs"
    mountpath:    "/var/lib/nova/instances"

# Enable extra yum repo
profile::base::yumrepo::repo_hash:
  rdo-release:
    ensure: present
#    exclude: 'openstack-neutron openstack-neutron-common python-neutron'
  calico37:
    ensure: absent
    exclude: 'dnsmasq'
  calico312:
    ensure: present
    exclude: 'dnsmasq'
  ceph-%{hiera('ceph_version')}:
    ensure: present
  centos-qemu-ev:
    ensure: present
  epel: # should be absent when using RDO, but whitelist can also work
    ensure:   present
    includepkgs: 'collectd collectd-virt bird bird6 htop bash-completion-extras lttng-ust userspace-rcu libbabeltrace leveldb liboath py-bcrypt python2-bcrypt libsmbios smbios-utils-bin lldpd'

# Compute nodes need only admin and cinder keys
ceph::profile::params::client_keys:
  'client.admin':
    secret: "%{hiera('ceph_storage_client_admin_key')}"
    mode: '0600'
    user: 'ceph'
    group: 'ceph'
    cap_mon: 'allow *'
    cap_osd: 'allow *'
    cap_mds: 'allow *'
  'client.cinder':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'nova'
    group: 'nova'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'
  'client.cinder-hdd-ec':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=volumes_hdd_ec_meta, profile rbd pool=volumes_hdd_ec_data'
  'client.cinder-ssd-ec':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=volumes_ssd_ec_meta, profile rbd pool=volumes_ssd_ec_data'
  'client.cinder-dcache-hdd-ec':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=dcache_hdd_ec_meta, profile rbd pool=dcache_hdd_ec_data'
  'client.cinder-dcache-hdd-ec8':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=dcache_hdd_ec_meta8, profile rbd pool=dcache_hdd_ec_data8'
  'client.cinder-arccache-hdd-ec':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=arccache_hdd_ec_meta, profile rbd pool=arccache_hdd_ec_data'
  'client.cinder-arccache-hdd-ec41':
    secret: "%{hiera('ceph_storage_client_cinder_key')}"
    mode: '0600'
    user: 'cinder'
    group: 'cinder'
    cap_mon: 'profile rbd'
    cap_osd: 'profile rbd pool=arccache_hdd_ec_meta41, profile rbd pool=arccache_hdd_ec_data41'

profile::storage::cephclient::create_extraconf: true
profile::storage::ceph_extraconf::config:
  client_ecc_backend:
    'config_key'  : "client.cinder-hdd-ec/rbd_default_data_pool"
    'config_value': "volumes_hdd_ec_data"
  client_ssd_ecc_backend:
    'config_key'  : "client.cinder-ssd-ec/rbd_default_data_pool"
    'config_value': "volumes_ssd_ec_data"
  client_dcache_backend:
    'config_key'  : "client.cinder-dcache-hdd-ec/rbd_default_data_pool"
    'config_value': "dcache_hdd_ec_data"
  client_dcache_backend8:
    'config_key'  : "client.cinder-dcache-hdd-ec8/rbd_default_data_pool"
    'config_value': "dcache_hdd_ec_data8"
  client_arccache_backend:
    'config_key'  : "client.cinder-arccache-hdd-ec/rbd_default_data_pool"
    'config_value': "arccache_hdd_ec_data"
  client_arccache_backend41:
    'config_key'  : "client.cinder-arccache-hdd-ec41/rbd_default_data_pool"
    'config_value': "arccache_hdd_ec_data41"

# Use rsync, not ssh for migrate transport to support sparse images
nova::config::nova_config:
  DEFAULT/metadata_workers:
    value: '4'
  libvirt/remote_filesystem_transport:
    value: 'rsync'
  libvirt/live_migration_permit_auto_converge:
    value: 'true'

# Red Hat Subscription Management (RHSM)
profile::rhsm::subscription::manage: true
profile::rhsm::virtwho::manage:      true
